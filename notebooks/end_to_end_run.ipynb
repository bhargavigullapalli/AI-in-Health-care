{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run End-to-end Group Chat\n",
    "\n",
    "This Jupyter Notebook demonstrates the workflow for creating and managing a group chat using Semantic Kernel and custom modules. The notebook integrates various Azure components, such as Key Vault, Blob Storage, and OpenAI services, to facilitate secure and efficient communication. Key functionalities include:\n",
    "\n",
    "- Setting up environment variables and configurations for Azure services.\n",
    "- Initializing and managing chat contexts and data access layers.\n",
    "- Creating a group chat using custom modules and Azure credentials.\n",
    "- Processing chat messages asynchronously and invoking responses.\n",
    "- Exploring chat history and analyzing message content.\n",
    "\n",
    "This notebook serves as a comprehensive guide for implementing a scalable and secure group chat solution leveraging Azure's cloud capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "CUR_DIR = os.path.abspath(\"\")\n",
    "ROOT_DIR = os.path.dirname(CUR_DIR)\n",
    "SRC_DIR = os.path.join(ROOT_DIR, 'src')\n",
    "\n",
    "# Load python modules from the src directory\n",
    "sys.path.append(SRC_DIR)\n",
    "print(f\"Added {SRC_DIR} to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(os.path.join(SRC_DIR, '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import setup_logging\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app import create_app_context\n",
    "from data_models.chat_context import ChatContext\n",
    "\n",
    "\n",
    "app_ctx = create_app_context()\n",
    "chat_ctx = ChatContext(\"notebook-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import group_chat\n",
    "\n",
    "\n",
    "chat, context = group_chat.create_group_chat(app_ctx, chat_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import AgentGroupChat\n",
    "from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "\n",
    "\n",
    "async def process_chat(chat: AgentGroupChat, user_message: str) -> None:\n",
    "    \"\"\"Process group chat with the given user message.\"\"\"\n",
    "    await chat.add_chat_message(ChatMessageContent(role=AuthorRole.USER, content=user_message))\n",
    "    chat.is_complete = False\n",
    "\n",
    "    async for response in chat.invoke():\n",
    "        print(f\"# {response.role} - {response.name or '*'}: '{response.content}'\")\n",
    "        if chat.is_complete:\n",
    "            break\n",
    "\n",
    "\n",
    "def print_chat_history(chat: AgentGroupChat) -> None:\n",
    "    \"\"\"Print the chat history.\"\"\"\n",
    "    for message in chat.history:\n",
    "        print(f\"{message.name}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_chat(chat, \"Orchestrator: Prepare tumor board for Patient ID: patient_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_chat(chat, \"Orchestrator: proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to print the chat history\n",
    "# print_chat_history(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to proceed with the chat\n",
    "# await process_chat(chat, \"Orchestrator: proceed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
